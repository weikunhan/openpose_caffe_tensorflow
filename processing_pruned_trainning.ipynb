{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weikunhan/anaconda3/envs/openpose_caffe/lib/python3.6/site-packages/keras/models.py:255: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown entry in loss dictionary: \"weight_stage1_L1\". Only expected the following keys: ['activation_85', 'activation_92']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ff0d33306da2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;31m# start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmultisgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m model.fit_generator(train_di,\n",
      "\u001b[0;32m~/anaconda3/envs/openpose_caffe/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    614\u001b[0m                                      \u001b[0;34m'dictionary: \"'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\". '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m                                      \u001b[0;34m'Only expected the following keys: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m                                      str(self.output_names))\n\u001b[0m\u001b[1;32m    617\u001b[0m             \u001b[0mloss_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown entry in loss dictionary: \"weight_stage1_L1\". Only expected the following keys: ['activation_85', 'activation_92']"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\" .....................\n",
    "\n",
    "################################################################################\n",
    "# Author: Weikun Han <weikunhan@gmail.com>\n",
    "# Crate Date: 04/07/2018        \n",
    "# Update:\n",
    "# Reference: \n",
    "#    https://github.com/michalfaber/keras_Realtime_Multi-Person_Pose_Estimation\n",
    "################################################################################\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas\n",
    "import re\n",
    "import math\n",
    "sys.path.append(\"..\")\n",
    "from keras.models import load_model\n",
    "from ds_iterator import DataIterator\n",
    "from ds_generator_client import DataGeneratorClient\n",
    "from optimizers import MultiSGD\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, CSVLogger, TensorBoard\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.applications.vgg19 import VGG19\n",
    "import keras.backend as K\n",
    "\n",
    "# Please modify input path to locate you file\n",
    "KERAS_DIR = 'model/keras'\n",
    "\n",
    "batch_size = 10\n",
    "base_lr = 4e-5 # 2e-5\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "lr_policy =  \"step\"\n",
    "gamma = 0.333\n",
    "stepsize = 136106 #68053   // after each stepsize iterations update learning rate: lr=lr*gamma\n",
    "max_iter = 200000 # 600000\n",
    "\n",
    "# True = start data generator client, False = use augmented dataset file (deprecated)\n",
    "use_client_gen = True\n",
    "\n",
    "WEIGHTS_BEST = \"weights.best.h5\"\n",
    "TRAINING_LOG = \"training.csv\"\n",
    "LOGS_DIR = \"./logs\"\n",
    "\n",
    "def get_last_epoch():\n",
    "    data = pandas.read_csv(TRAINING_LOG)\n",
    "    return max(data['epoch'].values)\n",
    "\n",
    "\n",
    "# Setup input and output name\n",
    "keras_model_filename = 'openpose_pruned_50p.h5'\n",
    "\n",
    "# Load Keras model\n",
    "keras_model = os.path.join(KERAS_DIR, keras_model_filename)\n",
    "model = load_model(keras_model)\n",
    "\n",
    "# prepare generators\n",
    "\n",
    "if use_client_gen:\n",
    "    train_client = DataGeneratorClient(port=5555, host=\"localhost\", hwm=160, batch_size=10)\n",
    "    train_client.start()\n",
    "    train_di = train_client.gen()\n",
    "    train_samples = 52597\n",
    "\n",
    "    val_client = DataGeneratorClient(port=5556, host=\"localhost\", hwm=160, batch_size=10)\n",
    "    val_client.start()\n",
    "    val_di = val_client.gen()\n",
    "    val_samples = 2645\n",
    "else:\n",
    "    train_di = DataIterator(\"../dataset/train_dataset.h5\", data_shape=(3, 368, 368),\n",
    "                      mask_shape=(1, 46, 46),\n",
    "                      label_shape=(57, 46, 46),\n",
    "                      vec_num=38, heat_num=19, batch_size=batch_size, shuffle=True)\n",
    "    train_samples=train_di.N\n",
    "    val_di = DataIterator(\"../dataset/val_dataset.h5\", data_shape=(3, 368, 368),\n",
    "                      mask_shape=(1, 46, 46),\n",
    "                      label_shape=(57, 46, 46),\n",
    "                      vec_num=38, heat_num=19, batch_size=batch_size, shuffle=True)\n",
    "    val_samples=val_di.N\n",
    "\n",
    "# setup lr multipliers for conv layers\n",
    "lr_mult=dict()\n",
    "for layer in model.layers:\n",
    "\n",
    "    if isinstance(layer, Conv2D):\n",
    "\n",
    "        # stage = 1\n",
    "        if re.match(\"conv5_\\d_CPM.*\", layer.name):\n",
    "            kernel_name = layer.weights[0].name\n",
    "            bias_name = layer.weights[1].name\n",
    "            lr_mult[kernel_name] = 1\n",
    "            lr_mult[bias_name] = 2\n",
    "\n",
    "        # stage > 1\n",
    "        elif re.match(\"Mconv\\d_stage.*\", layer.name):\n",
    "            kernel_name = layer.weights[0].name\n",
    "            bias_name = layer.weights[1].name\n",
    "            lr_mult[kernel_name] = 4\n",
    "            lr_mult[bias_name] = 8\n",
    "\n",
    "        # vgg\n",
    "        else:\n",
    "           kernel_name = layer.weights[0].name\n",
    "           bias_name = layer.weights[1].name\n",
    "           lr_mult[kernel_name] = 1\n",
    "           lr_mult[bias_name] = 2\n",
    "\n",
    "# configure loss functions\n",
    "\n",
    "# euclidean loss as implemented in caffe https://github.com/BVLC/caffe/blob/master/src/caffe/layers/euclidean_loss_layer.cpp\n",
    "def eucl_loss(x, y):\n",
    "    return K.sum(K.square(x - y)) / batch_size / 2\n",
    "\n",
    "losses = {}\n",
    "losses[\"weight_stage1_L1\"] = eucl_loss\n",
    "losses[\"weight_stage1_L2\"] = eucl_loss\n",
    "losses[\"weight_stage2_L1\"] = eucl_loss\n",
    "losses[\"weight_stage2_L2\"] = eucl_loss\n",
    "losses[\"weight_stage3_L1\"] = eucl_loss\n",
    "losses[\"weight_stage3_L2\"] = eucl_loss\n",
    "losses[\"weight_stage4_L1\"] = eucl_loss\n",
    "losses[\"weight_stage4_L2\"] = eucl_loss\n",
    "losses[\"weight_stage5_L1\"] = eucl_loss\n",
    "losses[\"weight_stage5_L2\"] = eucl_loss\n",
    "losses[\"weight_stage6_L1\"] = eucl_loss\n",
    "losses[\"weight_stage6_L2\"] = eucl_loss\n",
    "\n",
    "# learning rate schedule - equivalent of caffe lr_policy =  \"step\"\n",
    "iterations_per_epoch = train_samples // batch_size\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = base_lr\n",
    "    steps = epoch * iterations_per_epoch\n",
    "\n",
    "    lrate = initial_lrate * math.pow(gamma, math.floor(steps/stepsize))\n",
    "\n",
    "    return lrate\n",
    "\n",
    "# configure callbacks\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "checkpoint = ModelCheckpoint(WEIGHTS_BEST, monitor='loss', verbose=0, save_best_only=False, save_weights_only=True, mode='min', period=1)\n",
    "csv_logger = CSVLogger(TRAINING_LOG, append=True)\n",
    "tb = TensorBoard(log_dir=LOGS_DIR, histogram_freq=0, write_graph=True, write_images=False)\n",
    "\n",
    "callbacks_list = [lrate, checkpoint, csv_logger, tb]\n",
    "\n",
    "# sgd optimizer with lr multipliers\n",
    "multisgd = MultiSGD(lr=base_lr, momentum=momentum, decay=0.0, nesterov=False, lr_mult=lr_mult)\n",
    "\n",
    "# start training\n",
    "model.compile(loss=losses, optimizer=multisgd, metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit_generator(train_di,\n",
    "                    steps_per_epoch=train_samples // batch_size,\n",
    "                    epochs=max_iter,\n",
    "                    callbacks=callbacks_list,\n",
    "                    #validation_data=val_di,\n",
    "                    #validation_steps=val_samples // batch_size,\n",
    "                    use_multiprocessing=False,\n",
    "                    initial_epoch=last_epoch\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
